<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
    <title>8th Workshop on Benchmarking Multi-Target Tracking: Towards Spatiotemporal Action Grounding in Videos</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
    <link rel="stylesheet" href="assets/css/main.css"/>
    <noscript>
        <link rel="stylesheet" href="assets/css/noscript.css"/>
    </noscript>
</head>
<body class="is-preload">

<!-- Wrapper -->
<div id="wrapper">

    <!-- Header -->
    <header id="header" class="alt">
        <span class="logo"><img src="images/thumbnail.svg" alt=""/></span>
        <div class="header-text">
            <h3>Towards Spatiotemporal Action Grounding in Videos: 8th Workshop on MOT25
            </h3>
            <p>In conjunction with the conference on ICCV 2025</p>
        </div>
    </header>

<!--    <div class="section1">-->
<!--        <div class="gif-wrapper">-->
<!--            <img src="images/top_video.gif" alt="" class="top_all" />-->
<!--        </div>-->
<!--    </div>-->
    <div class="image-with-caption">
        <img src="images/top_bar.png" alt="" class="top_all"/>
        <div class="caption_top">Dense Spatiotemporal Annotations. It shows the queries and corresponding object bounding boxes for each frame.
            The top shows the current frame number and a visual representation of the number of queries (using boxes),
            while the bottom displays the specific query content. The full video is available
            <a href="https://drive.google.com/file/d/1QwIMCfBDomkWa5Hyqe8dEzDPnRDa5kBd/view?usp=sharing">here</a>.</div>
    </div>




    <!-- Nav -->
    <nav id="nav">
        <ul>
            <li><a href="#intro" class="active">Workshop</a></li>
            <li><a href="#speakers-sec">Speakers</a></li>
            <li><a href="#first">Schedule</a></li>
            <li><a href="#second">Competition</a></li>
            <li><a href="#cta">Organizers</a></li>
            <!--							<li><a href="https://cvpr2022.thecvf.com/">CVPR 2022</a></li>-->

        </ul>
    </nav>

    <!-- Main -->
    <div id="main">
        <!-- Introduction -->
        <section id="intro" class="main">
            <div class="spotlight">
                <div class="content">
                    <header class="major">
                        <h2>Workshop</h2>
                    </header>

                    <p> Understanding actions in videos goes beyond recognizing appearances - it requires tracking
                        objects as they perform complex behaviors over time. While recent benchmarks have made strides
                        in temporal action localization and referring object tracking, they often treat these tasks in
                        isolation. A gap remains in evaluating models that must both localize and track multiple objects
                        based on action-driven, natural language queries.
                    </p>
                    <p>
                        At the 8th edition of the BMTT workshop, we focus on action-aware multi-object tracking, aiming
                        to bridge the divide between vision and language by introducing unified challenges that evaluate
                        both temporal localization and object tracking. We invite the community to examine whether
                        current models can reason about actions, follow fine-grained language instructions, and scale to
                        more complex, real-world scenes.
                    </p>
                    <p>
                        Through invited talks, the challenge, and open discussions, this workshop will explore the
                        limitations of existing vision-language models, analyze the performance of state-of-the-art
                        trackers and temporal localizers on action-based queries, and promote the development of more
                        robust, multimodal video understanding systems.
                    </p>

                    <div class="image-with-caption">
                        <img src="images/q3_bar.png" alt="" class="top_all"/>
                        <div class="caption_q3">User Instructed Spatiotemporal Detections. It shows the bounding boxes of the referred objects over time for a given set of queries.
                            The top shows the current frame number and the given set of queries, and the bottom shows the period when the object appears.
                            The full video is available <a href="https://drive.google.com/file/d/1jlccP9AJS523deQhb1a1yBIyz8KdK15o/view?usp=sharing">here</a>.</div>
                    </div>

                    <h3>Info</h3>
                    <p>
                    <table style="width:100%">
                        <tr>
                            <td>Time</td>
                            <td>October 19, 2025</td>
                        </tr>
                        <tr>
                            <td>Venue</td>
                            <td><a href="https://iccv.thecvf.com/">ICCV 2025 (Honolulu, Hawai'i)</a></td>

                        </tr>
                        <tr>
                            <td>Train data released</td>
                            <td>July 11th, 2025</td>
                        </tr>
                        <tr>
                            <td>Test data released</td>
                            <td>July 11th, 2025</td>
                        </tr>

                        <tr>
                            <td>Challenge submission deadline</td>
                            <td>September 19th, 2025</td>
                        </tr>
                        <tr>
                            <td>Technical report deadline</td>
                            <td>September 26th, 2025</td>
                        </tr>
                        <tr>
                            <td>Recordings</td>
                            <td>Will be available after the workshop!</td>
                        </tr>
                    </table>
                    </p>
                </div>

            </div>
        </section>

        <section id="speakers-sec" class="main special">
            <header class="major">
                <h2>Speakers</h2>
            </header>
            <div class="box alt">
                <div class="row gtr-uniform">
                    <div class="col-4"><a href='https://www.cs.cmu.edu/~katef/'><span class="image fit"><img
                            src="images/speakers/katerina_crop.jpg" alt=""/></span>
                        <p>Katerina Fragkiadaki (CMU)</p></a></div>
                    <div class="col-4"><a href='https://www.philkr.net/'><span class="image fit"><img
                            src="images/speakers/philipp.jpg" alt=""/></span>
                        <p>Philipp Krähenbühl (University of Texas)</p></a></div>
                    <div class="col-4"><a href='http://www.cs.cmu.edu/~deva/'><span class="image fit"><img
                            src="images/speakers/deva_crop.jpg" alt=""/></span>
                        <p>Deva Ramanan (CMU)</p></a></div>
                    <div class="col-2"></div>
<!--                    <div class="col-4"><a href='https://www.vision.rwth-aachen.de/person/1/'><span-->
<!--                            class="image fit"><img src="images/speakers/bastian_crop.jpg" alt=""/></span>-->
<!--                        <p>Bastian Leibe (RWTH)</p></a></div>-->
<!--                    <div class="col-4"><a href='https://alexander-kirillov.github.io/'><span class="image fit"><img-->
<!--                            src="images/speakers/alexander.jpg" alt=""/></span>-->
<!--                        <p>Alexander Kirillov (Thinking Machines)</p></a></div>-->
                </div>
            </div>
        </section>


        <!-- First Section -->
        <section id="first" class="main special">
            <header class="major">
                <h2>Schedule (EST)</h2>
            </header>
            <p class="content">
            <div class="table-wrapper">
                <table class="alt">
                    <thead>
                    <tr>
                        <th>Time</th>
                        <th>Title</th>
                        <th>Speaker</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>08:30-09:00 am</td>
                        <td>Workshop introduction and Presentation of the new MOT25 dataset</td>
                        <td>Organizers</td>
                    </tr>
                    <tr>
                        <td>09:00-09:30 am</td>
                        <td>Invited Talk 1</td>
                        <td>Katerina Fragkiadaki</td>
                    </tr>
                    <tr>
                        <td>09:30-09:50 am</td>
                        <td>Oral: Third place of MOT25</td>
                        <td>Organizers</td>
                    </tr>
                    <tr>
                        <td>09:50-10:20 am</td>
                        <td>Invited Talk 2</td>
                        <td>Philipp Krähenbühl</td>
                    </tr>
                    <tr>
                        <td>10:20-10:40 am</td>
                        <td>Oral: Second place of MOT25</td>
                        <td>Organizers</td>
                    </tr>
                    <tr>
                        <td>10:40-11:00 am</td>
                        <td>Coffee Break</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>11:00-11:30 am</td>
                        <td>Invited Talk 3</td>
                        <td>Deva Ramanan</td>
                    </tr>
                    <tr>
                        <td>11:50 am – 12:10 pm</td>
                        <td>Oral: Winner of MOT25</td>
                        <td>Organizers</td>
                    </tr>
                    <tr>
                        <td>11:50 am – 12:15 pm</td>
                        <td>Discussion, Closing remarks and Awards</td>
                        <td>All speakers and Organizers</td>
                    </tr>
                    </tbody>
                </table>
            </div>
            </p>
        </section>

        <!-- Second Section -->
        <section id="second" class="main special">
            <header class="major">
                <h2>Competition</h2>
                <p> In this edition of the workshop, we focus on the task that combines temporal localization and
                    multi-object tracking under action-based natural language queries. The key question we pose to the
                    community is: <i> Can current models accurately localize and track multiple objects, based solely on
                        complex, free-form action queries?</i>
                </p>
                <p>
                    To this end, we organize a challenging competition in which participants are asked to develop their
                    models to solve the task. The goal of the challenge is to launch a new unified task for localization
                    and tracking based on large-scale, manually annotated action descriptions.
                </p>
            </header>
            <p class="content">


            <h2>MOT25-StAG track</h2>
            <span class="image fit"><img width="100" src="images/svag.jpg" alt=""/></span>

            For this track, we integrated three existing datasets (<a href="https://arxiv.org/abs/2102.01558">OVIS</a>,
            <a href="https://arxiv.org/abs/2010.07548">MOT17</a> and <a
                href="https://arxiv.org/abs/2003.09003">MOT20</a>) and annotated them with a variety of action language
            queries into the <strong>MOT25-StAG</strong> dataset. Participants can use all annotations in MOT25-StAG and
            test their temporal window localization and bounding box tracking methods on the MOT25-StAG test set.
            <br/></br>

            <strong>Rules: </strong>
            <ul style="list-style-type:disc;">
                <li> Participants are allowed to train their models using <strong>open-source</strong> datasets.</li>
                <li> Participants have to provide a technical report and code, showing which datasets were used for training.
                </li>
                <li> Participants are required to use the MOT25-StAG test set for evaluation and upload their results to the server. Evaluations on any other validation set can be conducted locally using our provided codebase.
                </li>
            </ul>


            <br/></br>
            <strong>Dataset: </strong> Image sequences are available at the <a href="https://songbai.site/ovis/">OVIS
            website</a>, <a href="https://motchallenge.net/data/MOT17/">MOT17 website</a> and <a
                href="https://motchallenge.net/data/MOT20">MOT20 website</a>. All annotations can be found <a href="https://drive.google.com/drive/folders/1S-qfooO6VMbibmhWEnBLkzlCj0HF1WYY?usp=drive_link">here</a>.
            <br/>
            <strong>Baselines: </strong> Our baseline for spatial grounding is available <a href="https://github.com/Shuaicong97/TempRMOT">here</a>,
            and baseline for temporal grounding is available <a href="https://github.com/Shuaicong97/FlashVTG">here</a>.
            <br/>
            <strong>Metric: </strong> m-HIoU, HOTA, mIoU, R1@X, R5@X, R10@X
            <br/>
            <strong>Evaluation script: </strong> <a href="https://github.com/Shuaicong97/SVAGEval">Evaluation Toolkit</a>
            <br/>
            <strong>Test server: </strong> The test server will be made available in the Codabench website on July 21th.<br/></br>

            <hr/>
            For the challenge, we will award the three best benchmark submissions. Challenge winners will be invited to
            give a short presentation describing their approach at the workshop event.

            <hr/>
            <h3>Technical report format</h3>
            Please follow a <strong>two-column layout</strong> for your submission. The technical report should <strong>at
            most contain 4 pages</strong> including references. However, shorter reports of 2 pages are very welcome.
            Submissions are <strong>not blind</strong>, hence, please include all authors on the submission. Only
            participants with a submitted report are considered for the reward and to present on the workshop. Please
            <strong>make your challenge entry public</strong> once submitted and make it clear to which method the
            report belongs. All reports should be sent to Tanveer Hannan (hannan [at] dbs . ifi . lmu . de). <strong>The
            deadline is September 26th, 23:59 PST.</strong>
            <hr/>
            </p>
        </section>

        <!-- Get Started -->
        <section id="cta" class="main special">
            <header class="major">
                <h2>Organizers</h2>
                <div class="box alt">
                    <div class="row gtr-uniform">
                        <div class="col-4"><span class="image fit"><img src="images/organizers/tanveer.jpg"
                                                                        alt=""/></span>
                            <p>Tanveer Hannan (LMU/MCML)</p></div>
                        <div class="col-4"><span class="image fit"><img src="images/organizers/shuaicong.jpg"
                                                                        alt=""/></span>
                            <p>Shuaicong Wu (LMU)</p></div>
                        <div class="col-4"><span class="image fit"><img src="images/organizers/weber.jpg"
                                                                        alt=""/></span>
                            <p>Mark Weber (TUM)</p></div>
                        <div class="col-4"><span class="image fit"><img src="images/organizers/suprosanna_crop.jpg"
                                                                        alt=""/></span>
                            <p>Suprosanna Shit (UZH)</p></div>
                        <div class="col-4"><span class="image fit"><img src="images/organizers/rajat_crop.jpg" alt=""/></span>
                            <p>Rajat Koner (Amazon)</p></div>
                        <div class="col-4"><span class="image fit"><img src="images/organizers/jindong_crop.jpg"
                                                                        alt=""/></span>
                            <p>Jindong Gu (Google/Oxford)</p></div>
                        <div class="col-4"><span class="image fit"><img src="images/organizers/osep_cropped.jpg"
                                                                        alt=""/></span>
                            <p>Aljoša Ošep (NVIDIA)</p></div>
                        <div class="col-4"><span class="image fit"><img src="images/organizers/seidl.jpg"
                                                                        alt=""/></span>
                            <p>Thomas Seidl (LMU/MCML)</p></div>
                        <div class="col-4"><span class="image fit"><img src="images/organizers/lealtaixe_cropped.jpg"
                                                                        alt=""/></span>
                            <p>Laura Leal-Taixé (NVIDIA/TUM)</p></div>
                    </div>
                </div>
                <div class="box alt">
                    <div class="col-2"></div>
                    <div class="row gtr-uniform">
                        <div class="col-3"><span class="image fit"><img src="images/affiliations/lmu.png"
                                                                        alt=""/></span></div>
                        <div class="col-3"><span class="image fit"><img src="images/affiliations/mcml.jpg"
                                                                        alt=""/></span></div>
                        <div class="col-3"><span class="image fit"><img src="images/affiliations/tum.png"
                                                                        alt=""/></span></div>
                        <div class="col-3"><span class="image fit"><img src="images/affiliations/uzh.png"
                                                                        alt=""/></span></div>
                        <div class="col-3"><span class="image fit"><img src="images/affiliations/amazon.png"
                                                                        alt=""/></span></div>
                        <div class="col-3"><span class="image fit"><img src="images/affiliations/google.jpg"
                                                                        alt=""/></span></div>
                        <div class="col-3"><span class="image fit"><img src="images/affiliations/oxford.png"
                                                                        alt=""/></span></div>
                        <div class="col-3"><span class="image fit"><img src="images/affiliations/nvidia.png"
                                                                        alt=""/></span></div>
                    </div>
                </div>
            </header>


        </section>


    </div>

    <!-- Footer -->
    <footer id="footer">
        <p class="copyright">Template from: <a href="https://html5up.net">HTML5 UP</a>.</p>
    </footer>

</div>

<!-- Scripts -->
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/jquery.scrollex.min.js"></script>
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/browser.min.js"></script>
<script src="assets/js/breakpoints.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>

</body>
</html>