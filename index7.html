<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>7th Workshop on Benchmarking Multi-Target Tracking: How Far Can Synthetic Data Take us?</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<span class="logo"><img src="images/logo.svg" alt="" /></span>
						<h1>How Far Can Synthetic Data Take us? <br>7th Workshop on Benchmarking Multi-Target Tracking</h1>
						<p>In conjuction with the conference on Computer Vision and Pattern Recognition (CVPR) 2022</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">Workshop</a></li>
							<li><a href="#speakers-sec">Speakers</a></li>
							<li><a href="#first">Schedule</a></li>
							<li><a href="#second">Competition</a></li>
							<li><a href="#cta">Organizers</a></li>
							<li><a href="https://cvpr2022.thecvf.com/">CVPR 2022</a></li>

						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Workshop</h2>
										</header>
										
										<p> Synthetic data has the potential to enable the next generation of deep learning algorithms to thrive on unprecedented amounts of <it>free</it> labelled data
											 while avoiding privacy and dataset bias concerns. As recently shown in our <a href="https://arxiv.org/abs/2108.09518">MOTSynth</a> work, models trained on synthetic 
											 data can already achieve competitive performances when tested on real datasets.
											 
											</p>
										<p>

											At the 7th BMTT workshop we aim to bring the tracking community together to further explore the potential of synthetic data for MOT. We have an exciting line-up of speakers, and are
											 organizing two challenges aiming to advance the state-of-the-art in synthetic-to-real tracking.
										</p>
										<h3>Info</h3>
										<p>
										<table style="width:100%">
										  <tr>
										    <td>Time</td>
										    <td>June  20,  2022</td>
										  </tr>
										  <tr>
										    <td>Venue</td>
											<td><a href="https://iccv2021.thecvf.com/" >CVPR 2022 (New Orleans, Louisiana)</a></td>

										  </tr>
										  <tr>
										    <td>Train data released</td>
										    <td>February 21st, 2022</td>
										  </tr>
										  <tr>
										    <td>Test data released</td>
										    <td>March 21st, 2022</td>
										  </tr>

										  <tr>
										    <td>Challenge submission deadline</td>
										    <td>May 23rd, 2022</td>
										  </tr>
										  <tr>
										    <td>Technical report deadline</td>
										    <td>May 30th, 2022</td>
										  </tr>
										  <tr>
										    <td>Recordings</td>
										    <td>Will be available after the workshop!</td>
										  </tr>
										</table>
										</p>
									</div>

								</div>
							</section>

							<section id="speakers-sec" class="main special">
								<header class="major">
									<h2>Speakers</h2>
								</header>
								<div class="box alt">
									<div class="row gtr-uniform">
										<div class="col-4"><a href='https://adriengaidon.com/'><span class="image fit"><img src="images/speakers/adrien.png" alt="" /></span><p> Adrien Gaidon (Toyota Technological Institute)</p></a></div>
										<div class="col-4"><a href='https://www.bu.edu/cs/profiles/saenko/'><span class="image fit"><img src="images/speakers/saenko.jpeg" alt="" /></span><p> Kate Saenko (Brown University)</p></a></div>
										<div class="col-4"><a href='https://people.eecs.berkeley.edu/~malik/'><span class="image fit"><img src="images/speakers/malik.jpeg" alt="" /></span><p> Jitendra Malik (UC Berkeley)</p></a></div>
										<div class="col-4"><a href='https://imagine.enpc.fr/~varolg/'><span class="image fit"><img src="images/speakers/varol.jpeg" alt="" /></span><p> Gül Varol (ParisTech)</p></a></div>

										<div class="col-4"><a href='http://www.cs.cmu.edu/~deva/'><span class="image fit"><img src="images/speakers/deva_crop.jpg" alt="" /></span><p>Deva Ramanan (CMU/Argo AI)</p></a></div>
										
										<div class="col-4"><a href='https://www.linkedin.com/in/tadas-baltrusaitis-234b1234/'><span class="image fit"><img src="images/speakers/baltrusaitis.jpeg" alt="" /></span><p> Tadas Baltrusaitis (Microsoft)</p></a></div>
										<div class="col-4"><a href='https://www.linkedin.com/in/gil-elbaz-a7197858/'><span class="image fit"><img src="images/speakers/elbaz.jpeg" alt="" /></span><p> Gil Elbaz (Datagen)</p></a></div>

										
										<div class="col-4"><a href='https://aayushp.github.io/'><span class="image fit"><img src="images/speakers/prakash.jpeg" alt="" /></span><p> Aayush Prakash (Meta)</p></a></div>
										<div class="col-4"><a href='https://www.linkedin.com/in/ido-gattegno-20125745/'><span class="image fit"><img src="images/speakers/gattegno.jpeg" alt="" /></span><p> Ido Gattegno (Meta)</p></a></div>
										<div class="col-4"></div>
										<div class="col-4"><a href='https://www.linkedin.com/in/evyatar-bluzer-46605077/'><span class="image fit"><img src="images/speakers/bluzer.jpeg" alt="" /></span><p> Evyatar Bluzer (Meta)</p></a></div>
										
										
										

										<div class="col-4"></div>
									</div>
								</div>
							</section>



						<!-- First Section -->
							<section id="first" class="main special">
								<header class="major">
									<h2>Schedule (EST)</h2>
								</header>
								<p class="content">
									<p> TBD</p>
								</p>
							</section>

						<!-- Second Section -->
							<section id="second" class="main special">
								<header class="major">
									<h2>Competitions</h2>
									<p> For this workshop edition we aim to shift the focus of the tracking community towards synthetic data and ask the following
										question: <i> can we advance state-of-the-art methods in pedestrian tracking using only synthetic
										data?</i> 
										</p>
										<p>
										
										To this end, we organize two challenges in which we require participants to develop their models using
										our recently proposed <a href="https://arxiv.org/abs/2108.09518">MOTSynth dataset</a> as  <b> only </b>  source of training data, and evaluate them on real datasets.
									
									</p>

									<p>
										
										
									
									</p>




								</header>
								<p class="content">
				

									<h2>MOTSynth2MOT17 track</h2> 
									<span class="image fit"><img width="100" src="images/motsynth-teaser-mot-alt.png" alt="" /></span>
									For this track, participants can use all the annotation modalities in MOTSynth, and test their pedestrian bounding box tracking methods on the MOT17 test set, under 
									the private detections setting.
									<br/></br>

									<strong>Rules: </strong>
									<ul style="list-style-type:disc;">
										<li> Models cannot be trained on any real data, except for ImageNet. Pretraining with COCO (or any other real dataset) is <b>not</b> allowed.</li>
										<li> MOT17 public (or any other external) detections <b>cannot</b> be used. Detections need to be obtained from training on MOTSynth only. </li>
										<li> MOT17 training data <b>can only be used for validation</b> and <b>not for training or fine-tuning</b>.</li>
										<li> Participants will have to provide a technical report and code, showing that only the allowed training data was used.</li>
									  </ul>
									


							
									<br/></br>
									<strong>Dataset: </strong> Train and test sequences are avalible at the <a href="https://motchallenge.net/data/MOTSynth-MOT-CVPR22/">MOTChallenge website</a>.<br/>
									<strong>Baselines: </strong> Our baselines, pre-trained models, and helper code for the dataset are avalible <a href="https://github.com/dvl-tum/motsynth-baselines">here</a>. <br/>
									<strong>Metric: </strong> <a href="https://github.com/JonathonLuiten/TrackEval">HOTA</a> will be used to rank participants.<br/>
									<strong>Test server: </strong> The test server will be made available in the MOTChallenge website on March 21st.
									<br/></br>
									<hr />

									<h2>MOTSynth2MOTS20 track</h2> 
 									<span class="image fit"><img width="100" src="images/motsynth-teaser-mots-alt.png" alt="" /></span>
									 For this track, participants can use all the annotation modalities in MOTSynth, and test their tracking and segmentation methods on the MOTS20 (a.k.a. MOTSChallenge) test set.
									
									 <br/></br>
									 <strong>Rules: </strong>
									 <ul style="list-style-type:disc;">
										 <li> Models cannot be trained on any real data, except for ImageNet. Pretraining with COCO (or any other real dataset) is <b>not</b> allowed.</li>
										 <li> MOT17 public (or any other external) detections/masks <b>cannot</b> be used. Detections need to be obtained from training on MOTSynth only. </li>
										 <li> MOTS20 training data <b>can only be used for validation</b> and <b>not for training or fine-tuning</b>.</li>
										 <li> Participants will have to provide a technical report and code, showing that only the allowed training data was used.</li>
									   </ul>
									 
									
									 <br/></br>
									<strong>Dataset: </strong> Train and test sequences are avalible at the <a href="https://motchallenge.net/data/MOTSynth-MOTS-CVPR22/">MOTChallenge website</a>.<br/>
									<strong>Baselines: </strong> Our baselines, pre-trained models, and helper code for the dataset are avalible <a href="https://github.com/dvl-tum/motsynth-baselines">here</a>. <br/>
									<strong>Metric: </strong> <a href="https://github.com/JonathonLuiten/TrackEval">HOTA</a> will be used to rank participants. <br/>
									<strong>Test server: </strong> The test server will be made available in the MOTChallenge website on March 21st.
									 <hr />
									 For each challenge, we will award both the most innovative and best performing submissions. Challenge winners will receive a prize (to be announced), and will 
									 be asked to give a short presentation describing their approach at the workshop event.
									 
									<hr />

									<h3>Technical report format</h3>
									Please follow a <strong>two-column layout</strong> for your submission. The technical report should <strong>at most contain 4 pages</strong> including references. However, shorter reports of 2 pages are very welcome. Submissions are <strong>not blind</strong>, hence, please include all authors on the submission. Only participants with a submitted report are considered for the reward and to present on the workshop. Please <strong>make your challenge entry public</strong> once submitted and make it clear to which method the report belongs. All reports should be sent to Guillem Brasó (guillem.braso [at] tum . de). <strong>The deadline is May 30th, 11:59 PST.</strong>
									<hr />
						</p>
							</section>

						<!-- Get Started -->
							<section id="cta" class="main special">
								<header class="major">
									<h2>Organizers</h2>
									<div class="box alt">
										<div class="row gtr-uniform">
											<div class="col-4"><span class="image fit"><img src="images/organizers/braso.jpg" alt="" /></span><p>Guillem Brasó (TUM)</p></div>
											<div class="col-4"><span class="image fit"><img src="images/organizers/matteo.jpeg" alt="" /></span><p>Matteo Fabbri (UNIMORE/GoatAI)</p></div>
											<div class="col-4"><span class="image fit"><img src="images/organizers/osep_cropped.jpg" alt="" /></span><p>Aljoša Ošep (TUM/CMU)</p></div>											

											<div class="col-4"><span class="image fit"><img src="images/organizers/weber.jpg" alt="" /></span><p>Mark Weber (TUM)</p></div>
											<div class="col-4"><span class="image fit"><img src="images/organizers/dendorfer.jpg" alt="" /></span><p>Patrick Dendorfer (TUM)</p></div>
											<div class="col-4"><span class="image fit"><img src="images/organizers/orcun.jpeg" alt="" /></span><p> Orçun Cetintas (TUM)</p></div>

											<div class="col-4"><span class="image fit"><img src="images/organizers/simone.png" alt="" /></span><p>Simone Calderara (UNIMORE/GoatAI)</p></div>
											<div class="col-4"><span class="image fit"><img src="images/organizers/rita.png" alt="" /></span><p>Rita Cucchiara (UNIMORE/GoatAI)</p></div>
											<div class="col-4"><span class="image fit"><img src="images/organizers/lealtaixe_cropped.jpg" alt="" /></span><p>Laura Leal-Taixé (TUM/ArgoAI)</p></div>
										</div>
									</div>
									<div class="box alt">
										<div class="col-2"></div>
										<div class="row gtr-uniform">
											<div class="col-3"><span class="image fit"><img src="images/affiliations/tum.png" alt="" /></span></div>
											<div class="col-3"><span class="image fit"><img src="images/affiliations/unimore.png" alt="" /></span></div>
											<div class="col-3"><span class="image fit"><img src="images/affiliations/argo.png" alt="" /></span></div>
											<div class="col-3"><span class="image fit"><img src="images/affiliations/goatai.png" alt="" /></span></div>

										</div>
									</div>
								</header>
								

							</section>



					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">Template from: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>